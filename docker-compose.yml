services:
  flowise:
    image: flowiseai/flowise
    container_name: flowise
    restart: always
    environment:
      - PORT=${PORT}
      - PASSPHRASE=${PASSPHRASE}
      - FLOWISE_USERNAME=${FLOWISE_USERNAME}
      - FLOWISE_PASSWORD=${FLOWISE_PASSWORD}
      - DEBUG=${DEBUG}
      - DATABASE_PATH=${DATABASE_PATH}
      - APIKEY_PATH=${APIKEY_PATH}
      - SECRETKEY_PATH=${SECRETKEY_PATH}
      - LOG_LEVEL=${LOG_LEVEL}
      - LOG_PATH=${LOG_PATH}
    ports:
      - '${PORT}:${PORT}'
    volumes:
      - ~/.flowise:/root/.flowise
    command: /bin/sh -c "sleep 3; flowise start"

  llama:
    container_name: llama
    image: llama.cpp:latest
    build:
      context: llama
      dockerfile: llama/.devops/full-cuda.Dockerfile
    volumes:
      - "./models:/models"
    command: "-c ${CONTEXT_WINDOW} --host 0.0.0.0 -t 20 -ngl 80 -mg 1 --port 4096 --mlock -m /models/${MODEL_NAME}"
    ports:
      - "4096:4096"

    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]
